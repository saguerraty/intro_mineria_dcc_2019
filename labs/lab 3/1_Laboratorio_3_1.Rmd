---
title: "Laboratorio 3.1 - Clustering en R"
author: "Sebastián Guerrati      Bastián Inostroza"
date: "Octubre 2019"
output: 
  html_document: 
    theme: default
    toc: yes
---


# Instrucciones

1. Trabaje en equipos de dos personas. Salvo excepciones, no se corregirá entregas con menos de dos integrantes.

2. Modifique este archivo `.Rmd` agregando sus respuestas donde corresponda.

3. Para cada pregunta, cuando corresponda, **incluya el código fuente que utilizó para llegar a su respuesta**.

4. Al final de la clase, **genere un archivo HTML usando RStudio** y súbalo a U-Cursos.
   Basta con que uno de los integrantes haga la entrega. Si ambos hacen una entrega en U-Cursos, se revisará cualquiera de éstas.

# Laboratorio

Para este laboratorio usaremos el dataset de cantidad de denuncias por 100 mil habitantes por tipo de delito desde el año 2001 al 2016 en Chile (Fuente: http://www.seguridadpublica.gov.cl/estadisticas/tasa-de-denuncias-y-detenciones/delitos-de-mayor-connotacion-social-series-de-datos-2001-2017/). 

Cargue el siguiente dataset:

```{r}
data <- read.table('https://users.dcc.uchile.cl/~hsarmien/mineria/datasets/denuncias-2001-2016.txt')
```


```{r paquetes}
library(knitr)
library(ggplot2)
```

Utilizaremos sólo los datos del 2005. Ejecute las siguientes líneas de código para filtrar los datos:

```{r, eval=T, results=F}
anio <- 2005
x <- data[data$anio == anio, ]
rownames(x) <- x$comuna
x <- x[, c(-1, -2)]
head(x)
```


## Parte 1: Exploración de datos

**1.1.** Descripción de los datos: explique brevemente qué representa cada instancia (fila) y atributo (columna) y estime el número de observaciones que hay en el dataset.

R: 
- Cada fila representa la cantidad de denuncias por cada 100k habidantes para cada tipo de delito en una comuna y año en particular.
- Cada columna (excepto la de comuna y la de año) representa la cantidad de denuncias por 100 mil habitantes por tipo de delito. Las filas de comuna y año representan la comuna y año en que dichas denuncias fueron realizadas respectivamente.
- El numero de observaciones (denuncias cada 100k habitantes) es 38752
```{r}
dim(data)
5536*7
```

**1.2.** Genere un boxplot a partir de los datos y describa tres características de estos basándose en el gráfico. Por ejemplo, explicar sobre la mediana de los datos, distribución de los datos, entre otros.

```{r fig.width=12}

anio <- 2005
y <- data[data$anio == anio, ]
y <- y[, c(-1,-2)]

boxplot(y)
```


## Parte 2: K-means

**2.1.** Describa qué es lo que hace cada línea del código adjunto. ¿Varían los resultados para diferentes ejecuciones? Si su respuesta es sí, ¿por qué?

```{r eval=F}
km.out <- kmeans(x, 3, nstart = 5)
km.out$centers
```

**Respuesta:**
En el código sin modificar no está fijada la semilla para componentes aleatorias del código, por lo que el kernel de R va a usar la semilla por default la que es suceptible a cambios en el tiempo. Dado que la componente aleatoria va a cambiar y que el inicio del algoritmo de K-means tiene una componente aleatoria, el resultado de clustering va a ser distinto en cada vez que se ejecute la linea, en particular ya que K-means es un algoritmo sensible al punto de inicio y que el número de inicios que se está considerando (nstart = 5) es un número bajo, lo que en sí va a dar un resultado poco estable en la asignación de clusters.


**2.2.** ¿Cómo podría evitar los resultados variables de `K-means`?

**Respuesta:**
Hay dos cambios que se deberían hacer tanto para mejorar el resultado como para hacerlo replicable, estos son aumentar el número de Nstart (el número por default es 100) y fijar la semilla aleatoria para este script de R con set.seed(%algún número%)

**2.3.** ¿Cómo podría encontrar *outliers* en los datos usando `K-means`? Describa su propuesta y muestre un par de ejemplos con distintos números de clusters.


**Respuesta:**
No alcanzo a programar este aproach pero una manera (aunque poco eficiente) es: 
1- Encontrar una configuración estable de k-means y guardarla
2- definier un umbral de distancia
3- Correr K-means y guardar la posición de los centoides de la configuración guardada en 1
4- Crear un df temporal con la fila i faltante
5- Correr K-means con el df temporal
6- si la ubicación de los centroides cambia por un valor (utilizando algúna norma definida previamente) mayor al umbral definido, apgregar un flag de outlier a esa fila
7- volver a el paso 4 para los siguientes i del df
8- si i == Nrow() break
Los outlier son los que tienen el flag
```{r eval=F}

```

## Parte 3: Clustering Jerárquico Aglomerativo

**3.1** Usando el dataframe `x`, ejecute cada uno de los 3 métodos de clustering jerárquico: `complete`, `single` y `average`, y visualice los dendrogramas formados. Adjunte código necesario.
```{r fig.width=12}
hc.complete <- hclust(dist(x), method = "complete")
hc.single <- hclust(dist(x), method = "single")
hc.average <- hclust(dist(x), method = "average")

par(mfrow=c(1,3))
plot(hc.complete, main="Complete", xlab="", ylab="", cex=.9)
plot(hc.single, main="Single", xlab="", ylab="", cex=.9)
plot(hc.average, main="Average", xlab="", ylab="", cex=.9)
```

**3.2.** Indique dónde haría los cortes en cada método (complete, single, average) basado en un número K de clusters. Para visualizar los cortes complete el siguiente código para cada método y el K elegido en cada caso.
```{r fig.width=12}
#install.packages("factoextra")
library("factoextra")


# Complete

fviz_nbclust(x, FUN = hcut, method = "silhouette")
fviz_nbclust(x, FUN = hcut, method = "wss")


metodo = hc.complete
K = 5
plot(metodo)
rect.hclust(metodo, k = K, border = 2:5)

metodo = hc.average
plot(metodo)
rect.hclust(metodo, k = K, border = 2:5)

metodo = hc.single
plot(metodo)
rect.hclust(metodo, k = K, border = 2:5)
```

**3.3.** ¿Cómo sería posible encontrar *outliers* en los datos usando clustering jerárquico? Basta con que comente su respuesta.
Una forma de encontrar outlier es definir una número mínimo de puntos que debiesen haber en un cluster para ser considerados un conjunto aislado, luego configuro mi dendograma y calculo en número ideal de cluster utilizando algún metodo como el del codo o gapstatistic e itero:
1- definir k=2 y revisar si esque algún cluster tiene menos puntos que el número umbral.
2- Si el cluster tiene menos que el umbral etiqueto esos puntos como outlier
3- Aumento mi K++
4- Mi K es igual al K óptimo? Si es break:
5- volver al paso 2
6-repetir esta metodología con distintas métricas para ver que puntos se repiten con la etiqueta de outliers, los que se repiten son outlier.

**3.4.** Describa una ventaja y una desventaja de emplear cada uno de los métodos de clustering vistos en este laboratorio (k-means y jerárquico aglomerativo).

**Respuesta:** 
Una ventaja de emplear HC es que puedo tener información de diferencias entre los puntos dentro de un cluster y además se tiene información de la distancia de los clusters entre sí, donde mientras más arriba en la jerarquía esté el vínculo entre clusters, menos relación tiene un cluster con otro. La desventaja de HC es que es sensible a la métrica que se esté usando, por lo que se podría cometer un error al o selecionar la adecuada. 
La ventaja de K-means es que es escalable (con buen rendimiento) a datasets grandes, además es un método que tiene una traducción directa a visualización lo que hace que la interpretación sea más fácil. La desventaja de K-means es que depende de una componente aleatoria con lo que si no está configurado apropiadamente y uno tiene "mala suerte" podría tener una mala clusterización con poca info agregada, o info de mala calidad.

